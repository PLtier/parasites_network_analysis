{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community import modularity\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "directory = '/Users/sunechristiansen/sune/network_analysis/project/parasites_network_analysis/projections_with_backbonings/projection_parasites/resource_allocation_naive.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory)\n",
    "df.rename(columns={'src': 'source', 'trg': 'target', 'nij': 'weight'}, inplace=True)\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph with weights\n",
    "G = nx.from_pandas_edgelist(df, source='source', target='target', edge_attr='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging two dataframes:\n",
    "- we use locality from nodes.csv and edges.csv\n",
    "- groups we take from edges.csv and nodes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Part: extract most frequent localities from nodes and edges .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_csv = pd.read_csv('../data/nodes.csv')\n",
    "edges_csv = pd.read_csv('../data/edges.csv')\n",
    "nodes_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edges_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "parasites_df = nodes_csv[nodes_csv[\" is_host\"] == 0].copy()\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../', 'data'))\n",
    "from common_localities import locality_to_common_locality\n",
    "most_frequent_localities = []\n",
    "for id_parasite, parasite in parasites_df.iterrows():\n",
    "    animals_idx = list(edges_csv[edges_csv[\" target\"] == id_parasite][\"# source\"]) + list(\n",
    "        edges_csv[edges_csv[\"# source\"] == id_parasite][\" target\"]\n",
    "    )\n",
    "    most_frequent_locality = nodes_csv.loc[animals_idx][' locality'].mode().loc[0]\n",
    "    most_frequent_locality = locality_to_common_locality[most_frequent_locality]\n",
    "    most_frequent_localities.append(most_frequent_locality)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasites_df.drop(columns=[' full_name', ' locality', ' group', ' is_host', ' _pos'], inplace=True)\n",
    "parasites_df.rename(columns={\"# index\": \"nodes_index\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasites_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasites_df['most_frequent_locality'] = most_frequent_localities\n",
    "parasites_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part: extract correct groups of parasties and most frequent groups of animals species from original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('../data/helminths.csv', encoding='ISO-8859-1').sort_values(by='Host')\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mode(series: pd.Series) -> pd.Series:\n",
    "    # could be NaN, then just return it\n",
    "    if series.isnull().all():\n",
    "        return []\n",
    "    return series.mode().sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_hostgroup_original_data= original_data.groupby(by=['Parasite', 'group', 'Host', 'hostgroup'], dropna=False).size().reset_index().drop(columns=[0])\n",
    "# take the most frequent hostgroup for each parasite\n",
    "group_hostgroup_original_data= group_hostgroup_original_data.groupby(by=['Parasite', 'group'])['hostgroup'].agg(random_mode).to_frame().reset_index()\n",
    "group_hostgroup_original_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_hostgroup_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on the conditions\n",
    "merged_df = parasites_df.merge(\n",
    "    group_hostgroup_original_data,\n",
    "    left_on=[' name'],  # Columns in attribute_df\n",
    "    right_on=['Parasite'],  # Columns in extra_attributes\n",
    "    how='inner',  # Use 'inner' to keep only matching rows\n",
    ")\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df.drop(columns=['Parasite'], inplace=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('../data/final_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(files)):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('/Users/sunechristiansen/sune/network_analysis/project/parasites_network_analysis/projections_with_backbonings/' + files[i])\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'src': 'source', 'trg': 'target', 'nij': 'weight'}, inplace=True)\n",
    "\n",
    "    # Create the graph with weights\n",
    "    G = nx.from_pandas_edgelist(df, source='source', target='target', edge_attr='weight')\n",
    "\n",
    "    print(f'name: {files[i]}, edges: {len(G.edges(data=True))}, nodes: {len(G.nodes)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_all_df = pd.read_csv('resource_allocation_naive.csv')\n",
    "# Rename columns\n",
    "resource_all_df.rename(columns={'src': 'source', 'trg': 'target', 'nij': 'weight'}, inplace=True)\n",
    "\n",
    "# Create the graph with weights\n",
    "G = nx.from_pandas_edgelist(resource_all_df, source='source', target='target', edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\" # Find all connected components\n",
    "connected_components = list(nx.connected_components(filtered_G))\n",
    "\n",
    "# Find the largest connected component\n",
    "largest_component = max(connected_components, key=len)\n",
    "\n",
    "# Create a subgraph of the largest connected component\n",
    "largest_subgraph = filtered_G.subgraph(largest_component) \"\"\"\n",
    "\n",
    "# Print details\n",
    "print(f\"Number of nodes in largest component: {len(largest_subgraph.nodes)}\")\n",
    "print(f\"Number of edges in largest component: {len(largest_subgraph.edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force find highest modularity resource allocation plus threshold:\n",
    "for i in range(100):\n",
    "    threshold = i/100\n",
    "    # Get all edge weights\n",
    "    edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "    # Filter edges with weights above the threshold\n",
    "    filtered_edges = [(u, v, w) for (u, v), w in edge_weights.items() if w >= threshold]\n",
    "\n",
    "    # Create a new graph with the filtered edges\n",
    "    filtered_G = nx.Graph()  # Use nx.DiGraph() if the original graph is directed\n",
    "    filtered_G.add_weighted_edges_from(filtered_edges)\n",
    "\n",
    "    communities = greedy_modularity_communities(filtered_G, weight=None)\n",
    "\n",
    "    modularity_r = modularity(filtered_G, communities)\n",
    "    print(\"threshold: \", threshold, \" & modularity: \", modularity_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
