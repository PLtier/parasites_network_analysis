{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_of_pkl(folder):\n",
    "    ret = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.pkl'):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            with open(filepath, 'rb') as f:\n",
    "                o = pickle.load(f)\n",
    "                ret[filename] = o\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 1_resource_allocation_naive.pkl, Nodes: 9213, Edges: 15588\n",
      "Graph: 1_simple_disparity_filter.pkl, Nodes: 16075, Edges: 443701\n",
      "Graph: 2_resource_allocation_naive.pkl, Nodes: 12770, Edges: 42691\n",
      "Graph: 2_simple_disparity_filter.pkl, Nodes: 14911, Edges: 336278\n"
     ]
    }
   ],
   "source": [
    "graphs = create_dict_of_pkl('../data/trimmed_networks_pkl')\n",
    "for graph_name, graph in graphs.items():\n",
    "    print(f\"Graph: {graph_name}, Nodes: {graph.number_of_nodes()}, Edges: {graph.number_of_edges()}\")\n",
    "\n",
    "graph_data = create_dict_of_pkl('../data/communities_and_modularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average nij for 1_resource_allocation_naive.pkl 10.132492970329068\n",
      "average nij for 1_simple_disparity_filter.pkl 11.623410810433151\n",
      "average nij for 2_resource_allocation_naive.pkl 4.940271634763646\n",
      "average nij for 2_simple_disparity_filter.pkl 1.8015570450639053\n"
     ]
    }
   ],
   "source": [
    "for name, graph in graphs.items():\n",
    "    graph_nijs = [edge[2][\"nij\"] for edge in graph.edges(data=True)]\n",
    "    print(f\"average nij for {name}: {np.average(graph_nijs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Gini Impurity\n",
    "Get the proportion of matches in a community for each ground truth label for all communities and take the gini impurity divided by the maximum gini impurity it could reach (to normalize). At the end get the average of gini impurities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_ground_truth_communities(graph, label_attribute):\n",
    "    communities = defaultdict(set)\n",
    "\n",
    "    # Iterate over nodes and group them by the label\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if label_attribute in data:\n",
    "            label = data[label_attribute]\n",
    "            communities[label].add(node)\n",
    "        else:\n",
    "            raise ValueError(f\"Node {node} does not have the attribute '{label_attribute}'.\")\n",
    "\n",
    "    # Convert the grouped nodes into a list of sets\n",
    "    return list(communities.values())\n",
    "\n",
    "def calculate_average_gini_impurity_norm(graph, discovered_communities, ground_truth_communities):\n",
    "    # Create a mapping of nodes to ground truth labels\n",
    "    ground_truth_labels = {}\n",
    "    for i, ground_truth_community in enumerate(ground_truth_communities):\n",
    "        for node in ground_truth_community:\n",
    "            ground_truth_labels[node] = i  # Assign an integer label to each ground truth community\n",
    "\n",
    "    total_gini_impurity = 0\n",
    "    total_nodes = len(graph.nodes)\n",
    "\n",
    "    # Calculate Gini impurity for each discovered community\n",
    "    for community in discovered_communities:\n",
    "        # Get ground truth labels for nodes in this discovered community\n",
    "        labels = [ground_truth_labels[node] for node in community if node in ground_truth_labels]\n",
    "        if not labels:\n",
    "            continue\n",
    "\n",
    "        # Calculate label frequencies\n",
    "        label_counts = Counter(labels)\n",
    "        community_size = len(community)\n",
    "        proportions = [count / community_size for count in label_counts.values()]\n",
    "\n",
    "        # Gini impurity for this community\n",
    "        gini_impurity = 1 - sum(p**2 for p in proportions)\n",
    "\n",
    "        # Normalize the Gini impurity by its maximum value\n",
    "        k = len(label_counts)  # Number of unique labels\n",
    "        max_gini_impurity = 1 - 1 / k if k > 1 else 1  # Avoid division by zero for single-label communities\n",
    "        normalized_gini_impurity = gini_impurity / max_gini_impurity if max_gini_impurity > 0 else 0\n",
    "\n",
    "        # Weight by the size of the community\n",
    "        total_gini_impurity += len(community) * normalized_gini_impurity\n",
    "\n",
    "    # Average Gini impurity\n",
    "    average_gini_impurity = total_gini_impurity / total_nodes\n",
    "    return average_gini_impurity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label,Algorithm,Impurity\n",
      "most_frequent_locality,greedy_modularity,0.8\n",
      "most_frequent_locality,label_propagation,0.56\n",
      "most_frequent_locality,Louvain,0.74\n",
      "most_frequent_locality,Infomap,0.52\n",
      "parasite_group,greedy_modularity,0.81\n",
      "parasite_group,label_propagation,0.72\n",
      "parasite_group,Louvain,0.76\n",
      "parasite_group,Infomap,0.69\n",
      "most_frequent_animal_group,greedy_modularity,0.6\n",
      "most_frequent_animal_group,label_propagation,0.33\n",
      "most_frequent_animal_group,Louvain,0.4\n",
      "most_frequent_animal_group,Infomap,0.32\n"
     ]
    }
   ],
   "source": [
    "# loop through labels and algos\n",
    "community_labels = [\"most_frequent_locality\",\"parasite_group\",\"most_frequent_animal_group\"]\n",
    "discovery_algorithm_labels = [\"greedy_modularity\",\"label_propagation\",\"Louvain\",\"Infomap\"]\n",
    "print('Label,Algorithm,Impurity')\n",
    "for name,graph in graphs.items():\n",
    "    if name == '2_simple_disparity_filter.pkl':\n",
    "        largest_cc = max(nx.connected_components(graph), key=len)\n",
    "        largest_component = graph.subgraph(largest_cc).copy()\n",
    "        for label in community_labels:\n",
    "            ground_truth_communities = get_ground_truth_communities(graph,label)\n",
    "            for algorithm in discovery_algorithm_labels:\n",
    "                algorithm_communities = graph_data[name][algorithm]['communities']\n",
    "                print(f\"{label},{algorithm},{round(calculate_average_gini_impurity_norm(largest_component,algorithm_communities,ground_truth_communities),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut size\n",
    "\n",
    "Function returns the generalized normalized cut size for multiple partitions of nodes.\n",
    "\n",
    "The *generalized normalized cut size* is calculated as the sum of the cut sizes between\n",
    "all pairs of partitions, normalized by the reciprocal of their volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generalized_normalized_cut_size(G, partitions, weight=None):\n",
    "    # Ensure the partitions cover all nodes in the graph\n",
    "    all_nodes = set().union(*partitions)\n",
    "    if all_nodes != set(G.nodes):\n",
    "        raise ValueError(\"Partitions must include all nodes in the graph.\")\n",
    "\n",
    "    total_cut = 0\n",
    "    combinations_count = sum(1 for _ in itertools.combinations(partitions, 2))\n",
    "    # Iterate over all pairs of partitions\n",
    "    for S, T in itertools.combinations(partitions, 2):\n",
    "        num_cut_edges = nx.cut_size(G, S, T, weight=weight)\n",
    "        volume_S = nx.volume(G, S, weight=weight)\n",
    "        volume_T = nx.volume(G, T, weight=weight)\n",
    "        \n",
    "        # Avoid division by zero if any partition has zero volume\n",
    "        if volume_S == 0 or volume_T == 0:\n",
    "            raise ValueError(\"One of the partitions has zero volume, which is invalid.\")\n",
    "        \n",
    "        total_cut += num_cut_edges * ((1 / volume_S) + (1 / volume_T))\n",
    "\n",
    "    return total_cut/combinations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '2_simple_disparity_filter.pkl'\n",
    "graph = graphs[name]\n",
    "largest_cc = max(nx.connected_components(graph), key=len)\n",
    "largest_component = graph.subgraph(largest_cc).copy()\n",
    "discovery_algorithm_labels = [\"greedy_modularity\",\"label_propagation\",\"Louvain\",\"Infomap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algorithm \u001b[38;5;129;01min\u001b[39;00m discovery_algorithm_labels:\n\u001b[0;32m      2\u001b[0m     algorithm_communities \u001b[38;5;241m=\u001b[39m graph_data[name][algorithm][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommunities\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     cut_size \u001b[38;5;241m=\u001b[39m \u001b[43mgeneralized_normalized_cut_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlargest_component\u001b[49m\u001b[43m,\u001b[49m\u001b[43malgorithm_communities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgorithm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cut size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcut_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m, in \u001b[0;36mgeneralized_normalized_cut_size\u001b[1;34m(G, partitions, weight)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#combinations_length = sum(1 for _ in combinations)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Iterate over all pairs of partitions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m S, T \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcombinations(partitions, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     num_cut_edges \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     volume_S \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mvolume(G, S, weight\u001b[38;5;241m=\u001b[39mweight)\n\u001b[0;32m     15\u001b[0m     volume_T \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mvolume(G, T, weight\u001b[38;5;241m=\u001b[39mweight)\n",
      "File \u001b[1;32mc:\\Users\\andra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\utils\\backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[1;32mc:\\Users\\andra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\algorithms\\cuts.py:84\u001b[0m, in \u001b[0;36mcut_size\u001b[1;34m(G, S, T, weight)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_directed():\n\u001b[0;32m     83\u001b[0m     edges \u001b[38;5;241m=\u001b[39m chain(edges, nx\u001b[38;5;241m.\u001b[39medge_boundary(G, T, S, data\u001b[38;5;241m=\u001b[39mweight, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(weight \u001b[38;5;28;01mfor\u001b[39;00m u, v, weight \u001b[38;5;129;01min\u001b[39;00m edges)\n",
      "File \u001b[1;32mc:\\Users\\andra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\algorithms\\cuts.py:84\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_directed():\n\u001b[0;32m     83\u001b[0m     edges \u001b[38;5;241m=\u001b[39m chain(edges, nx\u001b[38;5;241m.\u001b[39medge_boundary(G, T, S, data\u001b[38;5;241m=\u001b[39mweight, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\networkx\\algorithms\\boundary.py:105\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m edges \u001b[38;5;28;01mif\u001b[39;00m (e[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m nset1) \u001b[38;5;241m^\u001b[39m (e[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m nset1))\n\u001b[0;32m    101\u001b[0m nset2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(nbunch2)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    103\u001b[0m     e\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m edges\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (e[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m nset1 \u001b[38;5;129;01mand\u001b[39;00m e[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m nset2) \u001b[38;5;129;01mor\u001b[39;00m (e[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m nset1 \u001b[38;5;129;01mand\u001b[39;00m e[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m nset2)\n\u001b[0;32m    106\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for algorithm in discovery_algorithm_labels:\n",
    "    algorithm_communities = graph_data[name][algorithm]['communities']\n",
    "    cut_size = generalized_normalized_cut_size(largest_component,algorithm_communities)\n",
    "    print(f'{algorithm} cut size: {cut_size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
